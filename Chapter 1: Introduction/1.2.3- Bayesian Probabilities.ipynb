{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist Statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequentist Statistics rely upon frequency in data. This approach takes into account the uncertainity through probabilty which is viewed in terms of random, repeatable events. Till this point of our content, we were on frequentist statistics, which is also widely known as classical interpretation of probability.\n",
    "\n",
    "Let's see an example of frequentist interpretation with an experiment of coin toss. Sampling size is fixed. We don't conduct out experiment for infinite number of times. Our objective is to find out or estimate the fairness of the coin, which is 0.5 for an ideal case.\n",
    "\n",
    "\n",
    "Suppose, we toss our coin for 100 times, and got 46 heads and 54 tails. We again conducted this experiment for 500 times and got results. The results are summarised in the table below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1bw9rOMg4Zv9Fvxy1wBWWt6Y_h0pXWc0r\" alt=\"sigmoid_graph\" height=\"425\" width=\"750\">\n",
    "</center>\n",
    "\n",
    "\n",
    "We know that a fair coin has the $0.5$ probability of getting heads. From the law of the large numbers, this is true when we repeat the experiment of tossing a coin for infinite times, which is not possible. But from the three experiments conducted, we can conclude that, though the difference between ideal number of heads to be seen and the actual number of heads seen increases as the number of tosses are increased, the ratio or proportion of number of heads to total numnber of tosses approaches $0.5$, as that of fair coin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just saw the frequentist interpretation of probability. So, what is the issue with this approach? \n",
    "\n",
    "The main issue or flaw of this approach is that the result of an experiment is dependent on the number of times the experiment is repeated. \n",
    "Well, in this coin toss experiment, the repetation of an experiment is not a big deal, we just require a coin and some time. But, is these repetations always doable?\n",
    "\n",
    "\n",
    "[1] Conisder an uncertain event, for example, whether the moon was once iiin its own orbit around the sun, or whether the Artic ice cap will have disappeared by the end of the century. These are not events that can be repeated, as we did for coin toss, in order to define the notion of probability. However, we have an idea of, for example, of how quickly we think ice is melting. If we now obtain a fresh evidence through scientists' or researchers' observation, we may revise our opinion on the rate of ice melt. In such experiments, we would like to quantify the expression of uncertainity and make precise revisions of uncertainity in the light of new evidences, as subsequently revise the decisions. And this methodology or approach, whatever we say, can all be achieved through the elegant, and very general, **Bayesian interpretation of probability.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Interpretaion of Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
