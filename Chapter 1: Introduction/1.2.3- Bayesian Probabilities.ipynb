{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist Statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequentist Statistics rely upon the frequency in data. This approach takes into account the uncertainty through probability which is viewed in terms of random, repeatable events. Till this point in our content, we were on frequentist statistics, which is also widely known as the classical interpretation of probability.\n",
    "\n",
    "Let's see an example of frequentist interpretation with an experiment of a coin toss. The sampling size is fixed. We don't conduct our experiment an infinite number of times. Our objective is to find out or estimate the fairness of the coin, which is 0.5 for an ideal case.\n",
    "\n",
    "\n",
    "Suppose, we toss our coin 100 times, and got 46 heads and 54 tails. We again conducted this experiment 500 times and got results. The results are summarised in the table below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1bw9rOMg4Zv9Fvxy1wBWWt6Y_h0pXWc0r\" alt=\"sigmoid_graph\" height=\"425\" width=\"750\">\n",
    "</center>\n",
    "\n",
    "\n",
    "We know that a fair coin has the $0.5$ probability of getting heads. From the law of large numbers, this is true when we repeat the experiment of tossing a coin for infinite times, which is not possible. But from the three experiments conducted, we can conclude that, though the difference between an ideal number of heads to be seen and the actual number of heads seen increases as the number of tosses are increased, the ratio or proportion of the number of heads to a total number of tosses approaches $0.5$, like that of a fair coin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "issue with this approach? \n",
    "\n",
    "The main issue or flaw of this approach is that the result of an experiment is dependent on the number of times the experiment is repeated. \n",
    "Well, in this coin toss experiment, the repetition of an experiment is not a big deal, we just require a coin and some time. But, are these repetitions always doable?\n",
    "\n",
    "\n",
    "[1] Consider an uncertain event, for example, whether the moon was once in its orbit around the sun, or whether the Arctic ice cap will have disappeared by the end of the century. These are not events that can be repeated, as we did for the coin toss, to define the notion of probability. However, we have an idea of, for example, how quickly we think ice is melting. If we now obtain fresh evidence through scientists' or researchers' observation, we may revise our opinion on the rate of ice melt. In such experiments, we would like to quantify the expression of uncertainty and make precise revisions of uncertainty in the light of new evidence, as subsequently revise the decisions. And this methodology or approach, whatever we say, can all be achieved through the elegant, and very general, **Bayesian interpretation of probability.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Interpretaion of Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this interpretation of Probability, we make use of probability to represent uncertainty. Moreover, this approach provides us the capability to update our beliefs or probabilities in the evidence of new data.\n",
    "\n",
    "If we use numerical values to represent degrees of belief, or say probability, then a set of axioms, which are the multiplicative rule and sum rule, completely encodes common-sense properties of such beliefs or probabilities. Cox(1946) showed this.\n",
    "\n",
    "In the field of Pattern Recognition too, the notion of probability is much useful. let's take for instance an example of Polynomial Curve Fitting. There, we applied the frequentist notion of probability to the random values of observed variables, $t$. There was an uncertainty in the model parameters and we strived to find the best set of values for the model parameters, which in this case weight, $w$. Nevertheless, we can now address the uncertainty of the model parameters, weights, $w$ with the use of probability theory.\n",
    "\n",
    "Before we begin to state the Bayes theorem, let's spend some time in the prerequisites. The most important prerequisite is to understand Conditional Probability. Let's go for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of conditional proability is crucial in understanding Bayes theorem. Let's"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
